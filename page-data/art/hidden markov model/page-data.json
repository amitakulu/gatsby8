{"componentChunkName":"component---src-templates-blog-post-js","path":"/art/hidden markov model/","result":{"data":{"markdownRemark":{"html":"<h3>now made a table which having input and output what we will get in the the learning prblem, the viterbi algorithm and the likelihood problems.</h3>\n<p>Sure! Here's a table summarizing the input and output for the learning problem, the Viterbi algorithm, and the likelihood problem in a hidden Markov model:</p>\n<table>\n<thead>\n<tr>\n<th>Problem</th>\n<th>Input</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Learning</td>\n<td>Observation sequence, initial guess for model parameters (optional)</td>\n<td>Estimated model parameters that maximize the likelihood of the observation sequence</td>\n</tr>\n<tr>\n<td>Viterbi algorithm</td>\n<td>Trained HMM model, observation sequence</td>\n<td>Most probable sequence of hidden states that generated the observation sequence</td>\n</tr>\n<tr>\n<td>Likelihood</td>\n<td>Trained HMM model, observation sequence</td>\n<td>Likelihood of the observation sequence given the model</td>\n</tr>\n</tbody>\n</table>\n<h2>I hope this table helps you understand the differences between these three problems in a hidden Markov model!\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 533px; \n                             width: 100%;\n                             height: 300px; \n                             overflow: hidden;\n                             display: flex;\n                             justify-content: center;\n                             align-items: center;\n                             margin-top: 16px; \n                             margin-bottom: 16px;\n                             \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 150%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAeABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAABAADAgX/xAAWAQEBAQAAAAAAAAAAAAAAAAACAQP/2gAMAwEAAhADEAAAARkeXO4204u8vkJ0CR//xAAaEAADAQEBAQAAAAAAAAAAAAAAAQIREhAx/9oACAEBAAEFArjVhh8XLOWKkbI7Wum/f//EABYRAQEBAAAAAAAAAAAAAAAAAAARAf/aAAgBAwEBPwHUVX//xAAYEQEAAwEAAAAAAAAAAAAAAAAAAQIREv/aAAgBAgEBPwGktRRy/8QAGxAAAgIDAQAAAAAAAAAAAAAAAAEQMhEhQXH/2gAIAQEABj8C8nOWyrKHBvRZm3P/xAAcEAEAAgIDAQAAAAAAAAAAAAABABEhQTFRYZH/2gAIAQEAAT8he7ELGZ9fZTW8EZPemXS0bLxN40zzH2D8JtiXEn//2gAMAwEAAgADAAAAEBweDf/EABcRAQEBAQAAAAAAAAAAAAAAAAEQMWH/2gAIAQMBAT8QI5DzH//EABoRAAICAwAAAAAAAAAAAAAAAAARAVEQIUH/2gAIAQIBAT8Q0rmMli2f/8QAHhABAQACAgIDAAAAAAAAAAAAAREAITFBUWFxgZH/2gAIAQEAAT8QjjRQ1BPnJKN7OzCFBfQzkI60LxMS1ZeNTBQveeLgK4EBpjMmGhzf3Z6xmB8NDNqnp0YVbd5Nluf/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/587adbbaeac17799d45ea83d9240aeb8/507e6/4.webp 200w,\n/static/587adbbaeac17799d45ea83d9240aeb8/28a80/4.webp 400w,\n/static/587adbbaeac17799d45ea83d9240aeb8/4ffb6/4.webp 533w\"\n              sizes=\"(max-width: 533px) 100vw, 533px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/587adbbaeac17799d45ea83d9240aeb8/37402/4.jpg 200w,\n/static/587adbbaeac17799d45ea83d9240aeb8/4cda9/4.jpg 400w,\n/static/587adbbaeac17799d45ea83d9240aeb8/e49d2/4.jpg 533w\"\n            sizes=\"(max-width: 533px) 100vw, 533px\"\n            type=\"image/jpeg\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/587adbbaeac17799d45ea83d9240aeb8/e49d2/4.jpg\"\n            alt=\"An example image\"\n            title=\"kutta chutiya\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n    </span></h2>\n<h3>then what is use of viterbi algo we are give the observed sequence and hmm model and we get the sequesnce of the hidden states then what is benefits of getting the sequence of the hidden states.</h3>\n<h1>ðŸ§ª Brass: An Alloy of Copper and Zinc ðŸ§ª</h1>\n<p>Brass, often seen in door knobs, jars, and decorative materials, is not an element but an alloy. It's a mixture of copper and zinc, both elemental metals with their own properties. When mixed, they create new 'flavours' of metals, much like mixing spices in a bowl.</p>\n<blockquote>\n<p>\"Brass is an alloy of copper and zinc, creating a new 'flavour' of metal.\"</p>\n</blockquote>\n<h2>ðŸ”¬ The Science Behind Brass ðŸ”¬</h2>\n<p>Copper atoms are quite large and are arranged in a neat sequence, like rows of trolley cars. This neat arrangement allows copper to be ductile, meaning it can smoothly change shape under pressure. This property makes copper ideal for making wires for electricity transmission cables.</p>\n<p>Zinc atoms, on the other hand, are slightly larger than copper atoms. When mixed with copper, zinc atoms act like misshapen trolley cars, obstructing the flow of copper atoms. This obstruction makes the alloy stronger and less likely to bend easily under pressure.</p>\n<blockquote>\n<p>\"The addition of zinc to copper creates an alloy that is stronger and less likely to deform under pressure.\"</p>\n</blockquote>\n<h2>ðŸ¥„ The Usefulness of Brass ðŸ¥„</h2>\n<p>Brass is used for many things, not just copper. The addition of zinc makes it stronger and less likely to bend easily under pressure. This makes brass ideal for making utensils or durable objects like door knobs, which undergo a lot of rough handling.</p>\n<blockquote>\n<p>\"Brass is a durable material ideal for making utensils and objects that undergo a lot of rough handling.\"</p>\n</blockquote>\n<h2>ðŸ’¡ The Beauty of Alloys ðŸ’¡</h2>\n<p>Alloys, like brass, are beautiful in their own way. They are created by mixing different elements to create a new material with unique properties. So, the next time you see brass, remember the beauty of alloys and the simple yellow metal.</p>\n<blockquote>\n<p>\"Alloys, like brass, are a testament to the beauty of mixing different elements to create a new material with unique properties.\"</p>\n</blockquote>\n<p>Remember, while pure gold is often quite soft and can deform easily under pressure, brass is a good survivor. It's a testament to the beauty of alloys and the simple yellow metal.</p>\n<p>The Viterbi algorithm is used to find the most probable sequence of hidden states that generated an observation sequence given a trained hidden Markov model. This can be useful in many applications where we want to infer the underlying hidden states from the observed data.</p>\n<p>For example, in speech recognition, the observed data is the speech signal and the hidden states represent the underlying phonemes or words. The Viterbi algorithm can be used to find the most probable sequence of phonemes or words that generated the speech signal given a trained hidden Markov model of speech.</p>\n<p>In bioinformatics, the Viterbi algorithm is used to find the most probable sequence of hidden states representing the structure of a protein or DNA sequence given an observation sequence of amino acids or nucleotides and a trained hidden Markov model of protein or DNA structure.</p>\n<p>In finance, the Viterbi algorithm can be used to find the most probable sequence of hidden states representing the underlying market conditions given an observation sequence of stock prices and a trained hidden Markov model of stock price movements.</p>\n<p>These are just a few examples of how the Viterbi algorithm can be used to infer the underlying hidden states from observed data in various applications.</p>\n<hr>\n<h3>let take and example of stock market now expaline me .</h3>\n<p>Sure! Here's an example of how the Viterbi algorithm can be used in finance to infer the underlying market conditions from stock prices:</p>\n<p>Suppose we have a hidden Markov model with two hidden states <code class=\"language-text\">B</code> and <code class=\"language-text\">S</code> representing <code class=\"language-text\">Bullish</code> and <code class=\"language-text\">Bearish</code> market conditions respectively. The model has multiple observation symbols representing different ranges of stock price changes. For example, the observation symbols could be <code class=\"language-text\">+3%</code>, <code class=\"language-text\">+2%</code>, <code class=\"language-text\">+1%</code>, <code class=\"language-text\">0%</code>, <code class=\"language-text\">-1%</code>, <code class=\"language-text\">-2%</code>, and <code class=\"language-text\">-3%</code> representing stock price changes in the ranges <code class=\"language-text\">[+3%, +inf)</code>, <code class=\"language-text\">[+2%, +3%)</code>, <code class=\"language-text\">[+1%, +2%)</code>, <code class=\"language-text\">[-1%, +1%)</code>, <code class=\"language-text\">[-2%, -1%)</code>, <code class=\"language-text\">[-3%, -2%)</code>, and <code class=\"language-text\">(-inf, -3%]</code> respectively.</p>\n<p>The transition probabilities between the hidden states represent the probabilities of transitioning between bullish and bearish market conditions. The emission probabilities of the observation symbols given the hidden states represent the probabilities of observing different ranges of stock price changes given the market conditions. The initial state probabilities represent the prior probabilities of starting in bullish or bearish market conditions.</p>\n<p>Now suppose we have an observation sequence <code class=\"language-text\">O = [+2%, -1%, -3%, +1%, +3%]</code> representing daily stock price changes over a period of 5 days. We want to use the Viterbi algorithm to find the most probable sequence of hidden states representing the underlying market conditions that generated this observation sequence given a trained hidden Markov model of stock price movements.</p>\n<p>The Viterbi algorithm works by keeping track of the maximum probability of being in each hidden state at each time step given the observations up to that time step. This is done using a matrix <code class=\"language-text\">V</code> of size <code class=\"language-text\">(number of hidden states) x (length of observation sequence)</code>. The entry <code class=\"language-text\">V[i][t]</code> represents the maximum probability of being in hidden state <code class=\"language-text\">i</code> at time step <code class=\"language-text\">t</code> given the observations up to time step <code class=\"language-text\">t</code>.</p>\n<p>We start by initializing the first column of matrix <code class=\"language-text\">V</code> using the initial state probabilities and the emission probabilities of the first observation symbol. Then we iterate over the remaining columns of matrix <code class=\"language-text\">V</code>, updating each entry using the maximum probability of transitioning from any previous hidden state to the current hidden state and emitting the current observation symbol.</p>\n<p>After running the Viterbi algorithm, we obtain the most probable sequence of hidden states representing the underlying market conditions that generated the observation sequence. In this case, let's say that sequence is <code class=\"language-text\">[B, B, S, S, B]</code>.</p>\n<p>This sequence tells us that, according to our trained hidden Markov model, it is most probable that the market was in a bullish condition on days 1 and 2, then switched to a bearish condition on days 3 and 4, before switching back to a bullish condition on day 5.</p>\n<hr>\n<h3>now tell me use of likelihood problem explain me with example</h3>\n<p>Sure! Here's an example of how the likelihood problem can be used in a hidden Markov model:</p>\n<p>Suppose we have a hidden Markov model with two hidden states <code class=\"language-text\">H</code> and <code class=\"language-text\">C</code> representing <code class=\"language-text\">Hot</code> and <code class=\"language-text\">Cold</code> weather respectively. The model has two observation symbols <code class=\"language-text\">I</code> and <code class=\"language-text\">W</code> representing <code class=\"language-text\">Ice-cream</code> and <code class=\"language-text\">Woolen clothes</code> respectively. The transition probabilities between the hidden states represent the probabilities of transitioning between hot and cold weather. The emission probabilities of the observation symbols given the hidden states represent the probabilities of observing ice-cream or woolen clothes given the weather. The initial state probabilities represent the prior probabilities of starting in hot or cold weather.</p>\n<p>Now suppose we have an observation sequence <code class=\"language-text\">O = [I, W, W]</code>. We want to find the likelihood of this observation sequence given our trained hidden Markov model of weather.</p>\n<p>The likelihood of an observation sequence given a hidden Markov model is calculated using the forward algorithm. The forward algorithm works by keeping track of the probability of being in each hidden state at each time step given the observations up to that time step. This is done using a matrix <code class=\"language-text\">F</code> of size <code class=\"language-text\">(number of hidden states) x (length of observation sequence)</code>. The entry <code class=\"language-text\">F[i][t]</code> represents the probability of being in hidden state <code class=\"language-text\">i</code> at time step <code class=\"language-text\">t</code> given the observations up to time step <code class=\"language-text\">t</code>.</p>\n<p>We start by initializing the first column of matrix <code class=\"language-text\">F</code> using the initial state probabilities and the emission probabilities of the first observation symbol. Then we iterate over the remaining columns of matrix <code class=\"language-text\">F</code>, updating each entry using the sum of probabilities of transitioning from any previous hidden state to the current hidden state and emitting the current observation symbol.</p>\n<p>After running the forward algorithm, we obtain the likelihood of the observation sequence by summing the probabilities of being in any hidden state at the final time step.</p>\n<p>This likelihood tells us how probable it is to observe this particular sequence of observations given our trained hidden Markov model. This can be useful in many applications where we want to evaluate how well our model fits the observed data.</p>\n<p>For example, in speech recognition, we can use the likelihood to evaluate how well our trained hidden Markov model fits a given speech signal. In finance, we can use the likelihood to evaluate how well our trained hidden Markov model fits a given sequence of stock prices.</p>\n<p>I hope this example helps you understand how the likelihood problem can be used in a hidden Markov model!</p>\n<hr>\n<h3>In finance, we can use the likelihood to evaluate how well our trained hidden Markov model fits a given sequence of stock prices.explain it in simple terms.</h3>\n<p>Sure! In finance, we can use a hidden Markov model to represent the underlying market conditions that drive stock price movements. The hidden states in the model represent different market conditions, such as bullish or bearish, and the observation symbols represent different ranges of stock price changes.</p>\n<p>Once we have trained our hidden Markov model using historical stock price data, we can use the likelihood problem to evaluate how well our model fits a given sequence of stock prices. The likelihood problem calculates the probability of observing the given sequence of stock prices given our trained hidden Markov model.</p>\n<p>If the likelihood is high, it means that our model is able to accurately represent the underlying market conditions that generated the observed stock prices. If the likelihood is low, it means that our model is not able to accurately represent the underlying market conditions and may need to be retrained or improved.</p>\n<p>In simple terms, the likelihood problem allows us to evaluate how well our trained hidden Markov model is able to explain the observed stock price movements.</p>\n<hr>\n<h3>expalin me viterbi algorithm in hiddden markov model. take a simple example and do its numerical part. tell me wtha is input and what is output.</h3>\n<p>Sure! Here's a simple numerical example to help you understand how the Viterbi algorithm works:</p>\n<p>Suppose we have a hidden Markov model with two hidden states <code class=\"language-text\">H</code> and <code class=\"language-text\">C</code> representing <code class=\"language-text\">Hot</code> and <code class=\"language-text\">Cold</code> weather respectively. The model has two observation symbols <code class=\"language-text\">I</code> and <code class=\"language-text\">W</code> representing <code class=\"language-text\">Ice-cream</code> and <code class=\"language-text\">Woolen clothes</code> respectively. The transition probabilities between the hidden states are given by the matrix <code class=\"language-text\">A = [[0.7, 0.3], [0.4, 0.6]]</code>, where <code class=\"language-text\">A[i][j]</code> represents the probability of transitioning from state <code class=\"language-text\">i</code> to state <code class=\"language-text\">j</code>. The emission probabilities of the observation symbols given the hidden states are given by the matrix <code class=\"language-text\">B = [[0.1, 0.9], [0.8, 0.2]]</code>, where <code class=\"language-text\">B[i][j]</code> represents the probability of emitting observation symbol <code class=\"language-text\">j</code> when in hidden state <code class=\"language-text\">i</code>. The initial state probabilities are given by the vector <code class=\"language-text\">pi = [0.6, 0.4]</code>, where <code class=\"language-text\">pi[i]</code> represents the probability of starting in hidden state <code class=\"language-text\">i</code>.</p>\n<p>Now suppose we have an observation sequence <code class=\"language-text\">O = [I, W, W]</code>. We want to find the most probable sequence of hidden states that generated this observation sequence using the Viterbi algorithm.</p>\n<p>The Viterbi algorithm works by keeping track of the maximum probability of being in each hidden state at each time step given the observations up to that time step. This is done using a matrix <code class=\"language-text\">V</code> of size <code class=\"language-text\">(number of hidden states) x (length of observation sequence)</code>. The entry <code class=\"language-text\">V[i][t]</code> represents the maximum probability of being in hidden state <code class=\"language-text\">i</code> at time step <code class=\"language-text\">t</code> given the observations up to time step <code class=\"language-text\">t</code>.</p>\n<p>We start by initializing the first column of matrix <code class=\"language-text\">V</code> using the initial state probabilities and the emission probabilities of the first observation symbol:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">V[0][0] = pi[0] * B[0][O[0]] = 0.6 * 0.1 = 0.06\nV[1][0] = pi[1] * B[1][O[0]] = 0.4 * 0.8 = 0.32</code></pre></div>\n<p>Then we iterate over the remaining columns of matrix <code class=\"language-text\">V</code>, updating each entry using the maximum probability of transitioning from any previous hidden state to the current hidden state and emitting the current observation symbol:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">V[0][1] = max(V[0][0] * A[0][0], V[1][0] * A[1][0]) * B[0][O[1]] = max(0.06 * 0.7, 0.32 * 0.4) * 0.9 = 0.1296\nV[1][1] = max(V[0][0] * A[0][1], V[1][0] * A[1][1]) * B[1][O[1]] = max(0.06 * 0.3, 0.32 * 0.6) * 0.2 = 0.0384\n\nV[0][2] = max(V[0][1] * A[0][&lt;EUGPSCoordinates>], V[1][1] * A[1][&lt;EUGPSCoordinates>]) * B[&lt;EUGPSCoordinates>]</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">V[0][2] = max(V[0][1] * A[0][0], V[1][1] * A[1][0]) * B[0][O[2]] = max(0.1296 * 0.7, 0.0384 * 0.4) * 0.9 = 0.081648\nV[1][2] = max(V[0][1] * A[0][1], V[1][1] * A[1][1]) * B[1][O[2]] = max(0.1296 * 0.3, 0.0384 * 0.6) * 0.2 = 0.009216</code></pre></div>\n<p>The final matrix <code class=\"language-text\">V</code> is:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">[[0.06   , 0.1296 , 0.081648],\n [0.32   , 0.0384 , 0.009216]]</code></pre></div>\n<p>The most probable sequence of hidden states is obtained by backtracking through matrix <code class=\"language-text\">V</code> and choosing the hidden state with the maximum probability at each time step. In this case, the most probable sequence of hidden states is <code class=\"language-text\">[C, H, H]</code>.</p>\n<p>I hope this example helps you understand how the Viterbi algorithm works!</p>\n<hr>\n<h3>now give me an numerical example of likelihood problem with inpu and output.</h3>\n<p>Sure! Here's a simple numerical example to help you understand how to solve the likelihood problem in a hidden Markov model:</p>\n<p>Suppose we have the same hidden Markov model as in the previous example with two hidden states <code class=\"language-text\">H</code> and <code class=\"language-text\">C</code> representing <code class=\"language-text\">Hot</code> and <code class=\"language-text\">Cold</code> weather respectively, two observation symbols <code class=\"language-text\">I</code> and <code class=\"language-text\">W</code> representing <code class=\"language-text\">Ice-cream</code> and <code class=\"language-text\">Woolen clothes</code> respectively, transition probabilities given by the matrix <code class=\"language-text\">A = [[0.7, 0.3], [0.4, 0.6]]</code>, emission probabilities given by the matrix <code class=\"language-text\">B = [[0.1, 0.9], [0.8, 0.2]]</code>, and initial state probabilities given by the vector <code class=\"language-text\">pi = [0.6, 0.4]</code>.</p>\n<p>Now suppose we have an observation sequence <code class=\"language-text\">O = [I, W, W]</code>. We want to find the likelihood of this observation sequence given the model.</p>\n<p>The likelihood of an observation sequence given a hidden Markov model is calculated using the forward algorithm. The forward algorithm works by keeping track of the probability of being in each hidden state at each time step given the observations up to that time step. This is done using a matrix <code class=\"language-text\">F</code> of size <code class=\"language-text\">(number of hidden states) x (length of observation sequence)</code>. The entry <code class=\"language-text\">F[i][t]</code> represents the probability of being in hidden state <code class=\"language-text\">i</code> at time step <code class=\"language-text\">t</code> given the observations up to time step <code class=\"language-text\">t</code>.</p>\n<p>We start by initializing the first column of matrix <code class=\"language-text\">F</code> using the initial state probabilities and the emission probabilities of the first observation symbol:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">F[0][0] = pi[0] * B[0][O[0]] = 0.6 * 0.1 = 0.06\nF[1][0] = pi[1] * B[1][O[0]] = 0.4 * 0.8 = 0.32</code></pre></div>\n<p>Then we iterate over the remaining columns of matrix <code class=\"language-text\">F</code>, updating each entry using the sum of probabilities of transitioning from any previous hidden state to the current hidden state and emitting the current observation symbol:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">F[0][1] = (F[0][0] * A[0][&lt;EUGPSCoordinates>] + F[1][&lt;EUGPSCoordinates>] * A[1][&lt;EUGPSCoordinates>]) * B[&lt;EUGPSCoordinates>][O[1]] = (0.06 * 0.7 + 0.32 * 0.4) * 0.9 = 0.162\nF[1][1] = (F[&lt;EUGPSCoordinates>][&lt;EUGPSCoordinates>] * A[&lt;EUGPSCoordinates>][&lt;EUGPSCoordinates>] + F[&lt;EUGPSCoordinates>][&lt;EUGPSCoordinates>] * A[&lt;EUGPSCoordinates>][&lt;EUGPSCoordinates>]) * B[&lt;EUGPSCoordinates>][O[1]] = (0.06 * 0.3 + 0.32 * 0.6) * 0.2 = 0.0432\n\nF[&lt;EUGPSCoordinates>][2] = (F[&lt;EUGPSCoordinates>][1] * A[&lt;EUGPSCoordinates>][&lt;EUGPSCoordinates>] + F[1][1] * A[1][&lt;EUGPSCoordinates>]) * B</code></pre></div>\n<p>Sure! Continuing from where we left off:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">F[0][2] = (F[0][1] * A[0][0] + F[1][1] * A[1][0]) * B[0][O[2]] = (0.162 * 0.7 + 0.0432 * 0.4) * 0.9 = 0.10818\nF[1][2] = (F[0][1] * A[0][1] + F[1][1] * A[1][1]) * B[1][O[2]] = (0.162 * 0.3 + 0.0432 * 0.6) * 0.2 = 0.012312</code></pre></div>\n<p>The final matrix <code class=\"language-text\">F</code> is:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">[[0.06   , 0.162  , 0.10818 ],\n [0.32   , 0.0432 , 0.012312]]</code></pre></div>\n<p>The likelihood of the observation sequence is obtained by summing the probabilities of being in any hidden state at the final time step:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">P(O | model) = F[0][2] + F[1][2] = 0.10818 + 0.012312 = 0.120492</code></pre></div>\n<p>So the likelihood of the observation sequence <code class=\"language-text\">O = [I, W, W]</code> given the model is <code class=\"language-text\">0.120492</code>.</p>\n<p>I hope this example helps you understand how to solve the likelihood problem in a hidden Markov model!</p>\n<hr>\n<h3>now give me now numerical example of the learning problem.</h3>\n<p>Sure! Here's a simple numerical example to help you understand how to solve the learning problem in a hidden Markov model:</p>\n<p>Suppose we have a hidden Markov model with two hidden states <code class=\"language-text\">H</code> and <code class=\"language-text\">C</code> representing <code class=\"language-text\">Hot</code> and <code class=\"language-text\">Cold</code> weather respectively. The model has two observation symbols <code class=\"language-text\">I</code> and <code class=\"language-text\">W</code> representing <code class=\"language-text\">Ice-cream</code> and <code class=\"language-text\">Woolen clothes</code> respectively. We don't know the transition probabilities between the hidden states, the emission probabilities of the observation symbols given the hidden states, or the initial state probabilities.</p>\n<p>Now suppose we have an observation sequence <code class=\"language-text\">O = [I, W, W]</code>. We want to estimate the model parameters that maximize the likelihood of this observation sequence.</p>\n<p>The learning problem in a hidden Markov model is solved using the Baum-Welch algorithm. The Baum-Welch algorithm is an iterative algorithm that starts with an initial guess for the model parameters and iteratively updates the parameters to maximize the likelihood of the observation sequence.</p>\n<p>Let's start with an initial guess for the model parameters:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">A = [[0.5, 0.5], [0.5, 0.5]]\nB = [[0.5, 0.5], [0.5, 0.5]]\npi = [0.5, 0.5]</code></pre></div>\n<p>The Baum-Welch algorithm works by calculating two matrices <code class=\"language-text\">F</code> and <code class=\"language-text\">B</code> using the forward and backward algorithms respectively. The forward algorithm calculates the probability of being in each hidden state at each time step given the observations up to that time step, while the backward algorithm calculates the probability of observing the remaining observations given each hidden state at each time step.</p>\n<p>We start by running the forward algorithm to calculate matrix <code class=\"language-text\">F</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">F[0][0] = pi[0] * B[0][O[0]] = 0.5 * 0.5 = 0.25\nF[1][0] = pi[1] * B[1][O[0]] = 0.5 * 0.5 = 0.25\n</code></pre></div>\n<hr>","frontmatter":{"title":"aws","author":"Author Name","time":"25/02/2017","summary":"This is a summary of the blog post...abc def gfhy iiii olol olkmn lokjmm kijuyhgtr nhbngvfdc ","article":"Free","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAMBAgT/xAAWAQEBAQAAAAAAAAAAAAAAAAADAgT/2gAMAwEAAhADEAAAAU1hmdEGMSP/xAAbEAABBAMAAAAAAAAAAAAAAAABAAIDERATIf/aAAgBAQABBQKrGtqIAUeHHv8A/8QAFREBAQAAAAAAAAAAAAAAAAAAEBH/2gAIAQMBAT8Bh//EABYRAAMAAAAAAAAAAAAAAAAAAAABEf/aAAgBAgEBPwFOEP/EABsQAAIBBQAAAAAAAAAAAAAAAAABEQIQMTNx/9oACAEBAAY/AoZhGuRcKrf/xAAaEAEBAAMBAQAAAAAAAAAAAAABABExcUFh/9oACAEBAAE/IcC0fS4J2ewdCapWOqntv//aAAwDAQACAAMAAAAQkz//xAAXEQEBAQEAAAAAAAAAAAAAAAABABEh/9oACAEDAQE/EEPbb//EABkRAQACAwAAAAAAAAAAAAAAAAEAEVGR8P/aAAgBAgEBPxBVRBZ7c//EAB8QAQABAgcBAAAAAAAAAAAAAAEAESExQVFhgZHB0f/aAAgBAQABPxAUVS1fCIK3LFyfZYLalmOQRCi7xKa1AEOoVAXM/9k="},"images":{"fallback":{"src":"/static/587adbbaeac17799d45ea83d9240aeb8/7ffa0/4.jpg","srcSet":"/static/587adbbaeac17799d45ea83d9240aeb8/2d143/4.jpg 133w,\n/static/587adbbaeac17799d45ea83d9240aeb8/30567/4.jpg 267w,\n/static/587adbbaeac17799d45ea83d9240aeb8/7ffa0/4.jpg 533w","sizes":"(min-width: 533px) 533px, 100vw"},"sources":[{"srcSet":"/static/587adbbaeac17799d45ea83d9240aeb8/62dd2/4.webp 133w,\n/static/587adbbaeac17799d45ea83d9240aeb8/d32ea/4.webp 267w,\n/static/587adbbaeac17799d45ea83d9240aeb8/acf03/4.webp 533w","type":"image/webp","sizes":"(min-width: 533px) 533px, 100vw"}]},"width":800,"height":400.75046904315195}}}}}},"pageContext":{"slug":"/art/hidden markov model/"}},"staticQueryHashes":[],"slicesMap":{}}