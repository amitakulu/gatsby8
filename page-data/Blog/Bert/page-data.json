{"componentChunkName":"component---src-templates-blog-post-js","path":"/Blog/Bert/","result":{"data":{"markdownRemark":{"html":"<h3>BERT (Bidirectional Encoder Representations from Transformers)<a href=\"#bert--bidirectional-encoder-representations-from-transformers-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>Bert indeed utilizes the encoder part of the Transformer model. However, there are key differences in how BERT uses the Transformer encoder compared to the original Transformer architecture:</p>\n<h3>Transformer (Original) vs. BERT:<a href=\"#transformer--original--vs--bert-\" class=\"anchor\">ðŸ”—</a></h3>\n<ol>\n<li>\n<p><strong>Architecture</strong>:</p>\n<ul>\n<li><strong>Transformer</strong>: Comprises both an encoder and a decoder. The encoder processes input sequences (like sentences in translation tasks), and the decoder generates output sequences.</li>\n<li><strong>BERT</strong>: Uses only the encoder part of the Transformer. It doesn't require a decoder because BERT is designed for tasks like sentence classification, named entity recognition, and question answering, where generation of sequences (like in translation) is not the primary goal.</li>\n</ul>\n</li>\n<li>\n<p><strong>Training Objective</strong>:</p>\n<ul>\n<li><strong>Transformer</strong>: Trained for sequence-to-sequence tasks, such as machine translation. The encoder processes an input sequence, and the decoder generates a corresponding output sequence.</li>\n<li><strong>BERT</strong>: Trained using a masked language model objective. During training, some words in a sentence are masked (hidden), and the model is trained to predict these masked words using the surrounding context. This allows BERT to learn bidirectional representations.</li>\n</ul>\n</li>\n<li>\n<p><strong>Output</strong>:</p>\n<ul>\n<li><strong>Transformer</strong>: The output of the encoder is a sequence of vectors (one for each word or token in the input), which is then used by the decoder to generate an output sequence.</li>\n<li><strong>BERT</strong>: The output is also a sequence of vectors, but these vectors are used directly for downstream tasks. For example, in sentence classification, the vector corresponding to the special [CLS] token is used as the representation of the entire sentence.</li>\n</ul>\n</li>\n</ol>\n<h3>For your sentence: \"I will go to Austria.\"<a href=\"#for-your-sentence---i-will-go-to-austria--\" class=\"anchor\">ðŸ”—</a></h3>\n<ul>\n<li>\n<p>If you input this to the Transformer encoder (or BERT), you'd start by converting each word into an embedding, resulting in the input matrix <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span></span> with each word represented row-wise.</p>\n</li>\n<li>\n<p>After processing this sentence through the encoder layers, you'd get an output matrix (let's call it <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span></span></span></span></span>) where each row corresponds to a word in the sentence. This matrix captures the contextual information of each word in relation to every other word in the sentence.</p>\n</li>\n<li>\n<p>In BERT's case, if this sentence were used for a classification task, the first row of the matrix <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span></span></span></span></span> (corresponding to the [CLS] token, which would be prepended to the sentence) would be taken as the representation of the entire sentence and used for classification.</p>\n</li>\n</ul>\n<p>While BERT employs the Transformer's encoder structure, its application, training methodology, and utilization of the output differ significantly from the original Transformer model.</p>\n<hr>\n<h3>Architecture:<a href=\"#architecture-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>The architecture of the encoder in the original Transformer model and the architecture used in BERT are fundamentally the same. Both follow the structure you've outlined:</p>\n<ol>\n<li><strong>Word Embeddings &#x26; Positional Encoding</strong></li>\n<li><strong>Multi-Head Self-Attention</strong></li>\n<li><strong>Add &#x26; Normalize after Self-Attention</strong></li>\n<li><strong>Position-wise Feed-Forward Network (FFN)</strong></li>\n<li><strong>Add &#x26; Normalize after FFN</strong></li>\n</ol>\n<p>The key differences between the two are not in the architecture of the individual encoder layers but in how they are trained and used:</p>\n<ul>\n<li>\n<p><strong>Training Objective</strong>: BERT uses a masked language modeling objective, where a fraction of the input tokens are masked out, and the model is trained to predict those masked tokens. This is different from the typical training objectives of the original Transformer, such as sequence-to-sequence translation.</p>\n</li>\n<li>\n<p><strong>Bidirectionality</strong>: BERT stands for \"Bidirectional Encoder Representations from Transformers\". It is designed to capture context from both the left and the right of a token in its embeddings. This bidirectionality is achieved through its training method (masked language modeling) and not through changes in the encoder architecture itself.</p>\n</li>\n<li>\n<p><strong>Usage</strong>: BERT is pretrained on a large corpus and then fine-tuned on specific tasks. The original Transformer's encoder would typically be trained end-to-end for a specific task (like translation when paired with a decoder).</p>\n</li>\n<li>\n<p><strong>Number of Layers</strong>: While the architecture of each encoder layer remains the same, BERT often uses more layers (12 for BERT-base, 24 for BERT-large) compared to typical Transformer models used in tasks like translation.</p>\n</li>\n</ul>\n<p>The architecture of the encoder layers in both BERT and the original Transformer is the same. The differences lie in how they are trained and applied to tasks.</p>\n<hr>\n<h3>Idea of Bert:<a href=\"#idea-of-bert-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>Here's a detailed step-by-step of the process:</p>\n<ol>\n<li>\n<p><strong>Preprocessing &#x26; Masking</strong>:</p>\n<ul>\n<li>For the sentence \"let stick to the improvisation to this kit\", you decide to mask the word \"improvisation\".</li>\n<li>The input becomes something like: \"let stick to the [MASK] to this kit\".</li>\n</ul>\n</li>\n<li>\n<p><strong>Embeddings &#x26; Positional Encoding</strong>:</p>\n<ul>\n<li>Each word (including the [MASK] token) is converted into a dense vector using embeddings.</li>\n<li>Positional encodings are added to these embeddings to capture the sequence information.</li>\n</ul>\n</li>\n<li>\n<p><strong>Feeding to BERT</strong>:</p>\n<ul>\n<li>The sentence, with embeddings and positional encodings, is passed through BERT's layers (12 layers for BERT-base). At the end of these layers, you get an output matrix. Let's call this <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">Q</span></span></span></span></span> as you mentioned.</li>\n<li>From <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">Q</span></span></span></span></span>, you take the row corresponding to the [MASK] token. This row is a dense vector that contains BERT's best guess (in terms of embeddings) about what the masked word might be, given the context.</li>\n</ul>\n</li>\n<li>\n<p><strong>Additional FFNN &#x26; Softmax</strong>:</p>\n<ul>\n<li>This vector is passed through an additional Feed-Forward Neural Network (FFNN).</li>\n<li>Then, it's transformed into a probability distribution over the entire vocabulary using a softmax function. This gives you a probability score for every word in BERT's vocabulary.</li>\n</ul>\n</li>\n<li>\n<p><strong>Predicting the Masked Word</strong>:</p>\n<ul>\n<li>The word with the highest probability is chosen as BERT's prediction for the [MASK] token.</li>\n</ul>\n</li>\n</ol>\n<p>This masked language model training approach allows BERT to learn bidirectional representations because it's forced to predict a word based on its entire context (words before and after it) rather than just left-to-right or right-to-left.</p>\n<hr>\n<h2>Some Question and Answers:</h2>\n<h4>(To Understand Bert Fully)</h4>\n<h3>How does BERT utilize the masked language modeling objective during its training?<a href=\"#how-does-bert-utilize-the-masked-language-modeling-objective-during-its-training-\" class=\"anchor\">ðŸ”—</a></h3>\n<p><strong>Answer:</strong>\nWhen training BERT with the masked language modeling objective, the following steps are taken:</p>\n<ol>\n<li>The output matrix <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">Q</span></span></span></span></span> from BERT provides embeddings for every token in the input sequence.</li>\n<li>Only the embedding corresponding to the masked token (in <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">Q</span></span></span></span></span>) is taken and passed through the additional Feed-Forward Neural Network (FFNN).</li>\n<li>After the FFNN, the result is passed through a softmax layer to produce a probability distribution over the entire vocabulary.</li>\n<li>The prediction for the masked token is the word with the highest probability in this distribution.</li>\n</ol>\n<p>The ultimate goal during training is to adjust the model's weights so as to maximize the probability of the correct word (the original word before masking) in this distribution.</p>\n<hr>\n<h3>How is the loss for the masked language modeling task computed during BERT's training?<a href=\"#how-is-the-loss-for-the-masked-language-modeling-task-computed-during-bert-s-training-\" class=\"anchor\">ðŸ”—</a></h3>\n<p><strong>Answer:</strong>\nThe loss for the masked language modeling task in BERT's training is computed using the cross-entropy loss. Here's how it is derived:</p>\n<ol>\n<li>\n<p><strong>Predicted Probabilities</strong>: This is the output from the softmax layer. It provides a probability distribution over the entire vocabulary for the masked token.</p>\n</li>\n<li>\n<p><strong>True Label</strong>: The true label is represented as a one-hot encoded vector. In this vector, the position corresponding to the actual word (that was masked) is set to 1, while all other positions are set to 0. This representation effectively creates a \"probability distribution\" where the actual word has a probability of 1, and every other word has a probability of 0.</p>\n</li>\n</ol>\n<p>To compute the loss, the cross-entropy between the predicted probabilities and the true label is calculated. The objective during training is to minimize this loss. Minimizing the loss ensures that the predicted probability distribution comes as close as possible to the true label distribution.</p>\n<hr>\n<h3>How many words does BERT typically mask in a sentence during its training?<a href=\"#how-many-words-does-bert-typically-mask-in-a-sentence-during-its-training-\" class=\"anchor\">ðŸ”—</a></h3>\n<p><strong>Answer:</strong>\nDuring training, BERT randomly selects 15% of the words in each sequence to be masked.</p>\n<hr>\n<h3>What is the objective of BERT when it masks these words?<a href=\"#what-is-the-objective-of-bert-when-it-masks-these-words-\" class=\"anchor\">ðŸ”—</a></h3>\n<p><strong>Answer:</strong>\nThe model's objective is to predict the original identity of the masked words. It does this based on the context provided by the other, non-masked, words in the sequence.</p>\n<hr>\n<h3>Why does BERT mask multiple words in a sequence? What advantage does this bring to the training process?<a href=\"#why-does-bert-mask-multiple-words-in-a-sequence--what-advantage-does-this-bring-to-the-training-process-\" class=\"anchor\">ðŸ”—</a></h3>\n<p><strong>Answer:</strong>\nBy masking multiple words in a sequence, BERT's training process becomes more robust. This approach ensures the model learns to predict words from varying contexts and positions within a sentence. For example, if the sentence is \"The cat sat on the <strong>_</strong> and <strong>_</strong>.\", where the blanks could be \"mat\" and \"slept\", the model not only learns from the context \"The cat sat on the...\" but also from \"... and slept.\" This multi-word masking is a crucial feature that enables BERT to efficiently learn bidirectional representations from the text.</p>\n<hr>\n<h3>Apart from the Masked Language Model (MLM) objective, is there another training method used for BERT?<a href=\"#apart-from-the-masked-language-model--mlm--objective--is-there-another-training-method-used-for-bert-\" class=\"anchor\">ðŸ”—</a></h3>\n<p><strong>Answer:</strong>\nYes, besides the Masked Language Model (MLM) method, BERT incorporates a \"Next Sentence Prediction\" (NSP) task. This dual-task methodology was conceived to equip BERT with the capability to discern relationships between sentences, an essential skill for tasks like question answering and natural language inference.</p>\n<hr>\n<h3>Can you provide a detailed overview of BERT's two primary training methods?<a href=\"#can-you-provide-a-detailed-overview-of-bert-s-two-primary-training-methods-\" class=\"anchor\">ðŸ”—</a></h3>\n<p><strong>Answer:</strong>\nCertainly! BERT employs two primary training methods:</p>\n<ol>\n<li>\n<p><strong>Masked Language Model (MLM)</strong>:</p>\n<ul>\n<li>Random tokens in a sentence are masked out.</li>\n<li>BERT's challenge is to predict these masked tokens using their surrounding context as a reference.</li>\n</ul>\n</li>\n<li>\n<p><strong>Next Sentence Prediction (NSP)</strong>:</p>\n<ul>\n<li>For this task, BERT receives pairs of sentences during its training.</li>\n<li>The objective is to ascertain whether the second sentence in the pair naturally follows the first sentence in the original text.</li>\n<li>In terms of data distribution, 50% of the time, the second sentence is a random one from the corpus. The remaining 50% of the time, it genuinely is the consecutive sentence.</li>\n<li>An important aspect of this is the [CLS] token, which is added at the beginning of the input. The final hidden state of this token serves as the aggregate representation, facilitating the prediction of the relationship between the paired sentences.</li>\n</ul>\n</li>\n</ol>\n<p>These dual objectives meld together in BERT's pre-training phase. Post this phase, the model undergoes fine-tuning for specific tasks, like classification or question answering, leveraging the pre-trained weights as a starting point.</p>\n<hr>\n<p>When BERT is pre-trained using both the MLM and NSP tasks, the two objectives are combined, but they operate on different parts of the output:</p>\n<ol>\n<li>\n<p><strong>Masked Language Modeling (MLM)</strong>:</p>\n<ul>\n<li>As mentioned, random tokens in the input are masked. BERT's goal is to predict the identity of these masked tokens.</li>\n<li>The model uses the representations of the [MASK] tokens from its output to predict the original words for the MLM task.</li>\n</ul>\n</li>\n<li>\n<p><strong>Next Sentence Prediction (NSP)</strong>:</p>\n<ul>\n<li>The model is provided with two sentences, A and B. These sentences can have masked tokens because of the MLM task.</li>\n<li>Despite the presence of masked tokens, the overall structure and content of the sentences remain largely intact, allowing BERT to learn sentence relationships.</li>\n<li>For the NSP task, BERT uses the representation of the special [CLS] token from its output. This representation is expected to capture the relationship between the two sentences.</li>\n<li>The model predicts whether sentence B is the next sentence after sentence A based on this [CLS] token representation.</li>\n</ul>\n</li>\n</ol>\n<p>So, while the sentences fed to BERT during pre-training might have masked tokens (because of the MLM objective), the NSP task does not directly predict the masked tokens. Instead, it focuses on the relationship between the two sentences, using the aggregate representation captured by the [CLS] token.</p>\n<h3>Example:<a href=\"#example-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>However, it's important to note that both tasks (MLM and NSP) are performed simultaneously on the same input data. So, while BERT is trying to predict whether sentence B follows sentence A (NSP task), it is also trying to predict masked tokens within those sentences (MLM task).</p>\n<p>To illustrate with your example:</p>\n<ol>\n<li>\n<p>You start with sentences A and B:</p>\n<ul>\n<li>A: \"let stick to the improvisation to this kit\"</li>\n<li>B: \"I will go to Austria\"</li>\n</ul>\n</li>\n<li>\n<p>You might mask some words for the MLM task:</p>\n<ul>\n<li>A: \"let stick to the [MASK] to this kit\"</li>\n<li>B: \"I will [MASK] to Austria\"</li>\n</ul>\n</li>\n<li>\n<p>BERT will then take this input and perform two tasks:</p>\n<ul>\n<li>Predict the masked tokens (MLM task)</li>\n<li>Predict whether sentence B follows sentence A (NSP task)</li>\n</ul>\n</li>\n</ol>\n<p>The [CLS] token's representation at the end of BERT's layers is used for the NSP task, while the representations of the [MASK] tokens are used for the MLM task.</p>\n<hr>\n<h3>Bert for Different Tasks:<a href=\"#bert-for-different-tasks-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>BERT is versatile and can be adapted for various NLP tasks. The original BERT paper demonstrates how to use BERT for different tasks by slightly modifying the model's architecture. Here are the primary tasks and how BERT is utilized for each:</p>\n<ol>\n<li>\n<p><strong>Sentence Classification</strong>:</p>\n<ul>\n<li><strong>Task</strong>: Given a single sentence, classify it into one of several categories.</li>\n<li><strong>Example</strong>: Sentiment analysis where a sentence is classified as positive, negative, or neutral.</li>\n<li><strong>Input</strong>: [CLS] + sentence + [SEP]</li>\n<li><strong>Output</strong>: The [CLS] token's representation is taken and fed into a softmax layer to produce the classification probabilities.</li>\n</ul>\n</li>\n<li>\n<p><strong>Sentence Pair Classification</strong>:</p>\n<ul>\n<li><strong>Task</strong>: Given two sentences, classify the relationship between them.</li>\n<li><strong>Example</strong>: Natural Language Inference (NLI) where two sentences are classified as entailment, contradiction, or neutral.</li>\n<li><strong>Input</strong>: [CLS] + sentence A + [SEP] + sentence B + [SEP]</li>\n<li><strong>Output</strong>: The [CLS] token's representation is taken and fed into a softmax layer to classify the relationship.</li>\n</ul>\n</li>\n<li>\n<p><strong>Question Answering</strong>:</p>\n<ul>\n<li><strong>Task</strong>: Given a passage and a question, determine the answer span within the passage.</li>\n<li><strong>Example</strong>: \"Question: What is the capital of France? Passage: Paris is the capital of France.\"</li>\n<li><strong>Input</strong>: [CLS] + question + [SEP] + passage + [SEP]</li>\n<li><strong>Output</strong>: Two vectors are produced from the representations of each token in the passage: one for the start and one for the end of the answer span. These vectors are used to determine the span of the answer in the passage.</li>\n</ul>\n</li>\n<li>\n<p><strong>Token-level Classification</strong>:</p>\n<ul>\n<li><strong>Task</strong>: Assign a label to each token in a sentence.</li>\n<li><strong>Example</strong>: Named Entity Recognition (NER) where tokens in a sentence are classified as person, organization, location, etc.</li>\n<li><strong>Input</strong>: [CLS] + sentence + [SEP]</li>\n<li><strong>Output</strong>: Each token's representation in the sentence is fed into a softmax layer to determine its label.</li>\n</ul>\n</li>\n<li>\n<p><strong>Sentence Pair Regression</strong>:</p>\n<ul>\n<li><strong>Task</strong>: Given two sentences, predict a continuous value representing their relationship.</li>\n<li><strong>Example</strong>: Semantic Textual Similarity (STS) where the model predicts how similar two sentences are on a scale.</li>\n<li><strong>Input</strong>: [CLS] + sentence A + [SEP] + sentence B + [SEP]</li>\n<li><strong>Output</strong>: The [CLS] token's representation is taken and fed into a dense layer to produce a continuous value.</li>\n</ul>\n</li>\n</ol>\n<p>For all these tasks, BERT's pre-trained weights serve as initialization, and the model is fine-tuned on task-specific data. The beauty of BERT lies in its ability to adapt to a wide range of tasks with minimal architectural changes, leveraging the rich representations it learned during pre-training.</p>\n<hr>\n<h3>Bert for Question Answering Task:<a href=\"#bert-for-question-answering-task-\" class=\"anchor\">ðŸ”—</a></h3>\n<h3>Training Phase:<a href=\"#training-phase-\" class=\"anchor\">ðŸ”—</a></h3>\n<ol>\n<li>\n<p><strong>Input Preparation</strong>:</p>\n<ul>\n<li>Given a passage and an associated question, the input is formed as: [CLS] + question + [SEP] + passage + [SEP].</li>\n<li>The ground truth answer is provided as a span in the passage.</li>\n</ul>\n</li>\n<li>\n<p><strong>BERT's Output</strong>:</p>\n<ul>\n<li>After passing the input through BERT, every token in the passage gets a representation.</li>\n<li>Two separate dense layers are applied to each token's representation: one predicts the start position and the other predicts the end position of the answer span.</li>\n</ul>\n</li>\n<li>\n<p><strong>Loss Calculation</strong>:</p>\n<ul>\n<li>The model's predicted start and end positions are compared to the ground truth start and end positions.</li>\n<li>Cross-entropy loss is calculated for both the start and end positions and is combined to form the total loss for the example.</li>\n</ul>\n</li>\n</ol>\n<h3>Example for Training:<a href=\"#example-for-training-\" class=\"anchor\">ðŸ”—</a></h3>\n<ul>\n<li><strong>Question</strong>: \"What is the capital of France?\"</li>\n<li><strong>Passage</strong>: \"Paris is the capital of France. It is known for the Eiffel Tower.\"</li>\n<li><strong>Ground Truth Answer</strong>: \"Paris\"</li>\n</ul>\n<p>The input to BERT would be: [CLS] What is the capital of France? [SEP] Paris is the capital of France. It is known for the Eiffel Tower. [SEP]</p>\n<p>The ground truth start position for the answer is the token \"Paris\", and the end position is the same token \"Paris\". The model will aim to predict these positions correctly during training.</p>\n<h3>Inference Phase:<a href=\"#inference-phase-\" class=\"anchor\">ðŸ”—</a></h3>\n<ol>\n<li>\n<p><strong>Input Preparation</strong>:</p>\n<ul>\n<li>Similar to training, the question and passage are passed as: [CLS] + question + [SEP] + passage + [SEP].</li>\n</ul>\n</li>\n<li>\n<p><strong>Predicting Start and End Positions</strong>:</p>\n<ul>\n<li>After processing the input, BERT provides a start position score and an end position score for each token in the passage.</li>\n<li>The token with the highest start position score is predicted as the start of the answer span, and the token with the highest end position score is predicted as the end of the answer span.</li>\n</ul>\n</li>\n<li>\n<p><strong>Extracting Answer</strong>:</p>\n<ul>\n<li>Once the start and end positions are identified, the tokens between (and including) these positions form the predicted answer.</li>\n</ul>\n</li>\n</ol>\n<h3>Example for Inference:<a href=\"#example-for-inference-\" class=\"anchor\">ðŸ”—</a></h3>\n<ul>\n<li><strong>Question</strong>: \"What is the capital of France?\"</li>\n<li><strong>Passage</strong>: \"Paris is the capital of France. It is known for the Eiffel Tower.\"</li>\n</ul>\n<p>After processing, if BERT predicts the start position as the token \"Paris\" and the end position as the same token \"Paris\", the predicted answer would be \"Paris\".</p>\n<p>In the Question Answering task with BERT, the main idea is predicting the correct start and end positions of the answer within the passage. The training ensures that these predictions are as accurate as possible, and during inference, these predictions are used to extract the answer span.</p>\n<hr>\n<h3>Bert for Sentence Pair Regression:<a href=\"#bert-for-sentence-pair-regression-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>Sentence Pair Regression involves predicting a continuous value based on the relationship between two sentences. This is commonly used in tasks like Semantic Textual Similarity (STS), where the objective is to measure the similarity between two sentences.</p>\n<h3>Training Phase:<a href=\"#training-phase-\" class=\"anchor\">ðŸ”—</a></h3>\n<ol>\n<li>\n<p><strong>Input Preparation</strong>:</p>\n<ul>\n<li>Two sentences are provided as input.</li>\n<li>The input is formed as: [CLS] + sentence A + [SEP] + sentence B + [SEP].</li>\n<li>A ground truth similarity score, typically ranging between 0 and 1 (or 0 and 5, depending on the dataset), is associated with this pair.</li>\n</ul>\n</li>\n<li>\n<p><strong>BERT's Output</strong>:</p>\n<ul>\n<li>After passing the input through BERT, the [CLS] token's representation is extracted.</li>\n<li>This representation is then passed through a dense layer to produce a continuous value (the predicted similarity score).</li>\n</ul>\n</li>\n<li>\n<p><strong>Loss Calculation</strong>:</p>\n<ul>\n<li>The predicted similarity score is compared to the ground truth similarity score.</li>\n<li>Mean Squared Error (MSE) or another regression loss is used to measure the difference between the predicted and true values.</li>\n</ul>\n</li>\n</ol>\n<h3>Example for Training:<a href=\"#example-for-training-\" class=\"anchor\">ðŸ”—</a></h3>\n<ul>\n<li><strong>Sentence A</strong>: \"A boy is playing soccer.\"</li>\n<li><strong>Sentence B</strong>: \"A child is playing football.\"</li>\n<li><strong>Ground Truth Similarity Score</strong>: 4.7 (out of 5)</li>\n</ul>\n<p>The input to BERT would be: [CLS] A boy is playing soccer. [SEP] A child is playing football. [SEP]</p>\n<p>BERT would process this input and produce a predicted similarity score, say 4.5. The loss would be calculated based on the difference between this predicted value (4.5) and the ground truth value (4.7).</p>\n<h3>Inference Phase:<a href=\"#inference-phase-\" class=\"anchor\">ðŸ”—</a></h3>\n<ol>\n<li>\n<p><strong>Input Preparation</strong>:</p>\n<ul>\n<li>Similar to training, the two sentences are passed as: [CLS] + sentence A + [SEP] + sentence B + [SEP].</li>\n</ul>\n</li>\n<li>\n<p><strong>Predicting Similarity Score</strong>:</p>\n<ul>\n<li>The [CLS] token's representation is extracted and passed through the dense layer to get the predicted similarity score.</li>\n</ul>\n</li>\n</ol>\n<h3>Example for Inference:<a href=\"#example-for-inference-\" class=\"anchor\">ðŸ”—</a></h3>\n<ul>\n<li><strong>Sentence A</strong>: \"A boy is playing soccer.\"</li>\n<li><strong>Sentence B</strong>: \"A child is playing a game.\"</li>\n</ul>\n<p>Based on the relationship between the sentences and the trained model, BERT might predict a similarity score of, say, 3.8.</p>\n<p>The goal in Sentence Pair Regression tasks like STS is to accurately predict the degree of similarity between two sentences, represented as a continuous value.</p>\n<hr>\n<p>BERT can be utilized as a powerful feature extractor for various NLP tasks. When you use BERT for feature extraction, you're essentially leveraging the embeddings (vector representations) that BERT produces for tokens or entire sentences. These embeddings capture rich semantic information, which can be used by downstream models or systems.</p>\n<h3>How to Use BERT for Feature Extraction:<a href=\"#how-to-use-bert-for-feature-extraction-\" class=\"anchor\">ðŸ”—</a></h3>\n<ol>\n<li><strong>Token Embeddings</strong>:\n<ul>\n<li>Given a sentence or text, you can extract the embeddings for individual tokens.</li>\n<li>Each token's embedding captures its contextual information within the sentence.</li>\n</ul>\n</li>\n<li><strong>Sentence Embeddings</strong>:\n<ul>\n<li>For a holistic representation of the entire sentence or text, one common approach is to use the embedding of the [CLS] token, especially after fine-tuning.</li>\n<li>Alternatively, you can average or pool the embeddings of individual tokens to get a single vector representation for the entire sentence.</li>\n</ul>\n</li>\n</ol>\n<h3>Example:<a href=\"#example-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>Let's consider the sentence: \"BERT is a powerful model.\"</p>\n<ol>\n<li>\n<p><strong>Token Embeddings</strong>:</p>\n<ul>\n<li>Feed the sentence into BERT.</li>\n<li>Extract the embeddings for each token (\"BERT\", \"is\", \"a\", \"powerful\", \"model\").</li>\n<li>These embeddings can be used for tasks like Named Entity Recognition, POS tagging, etc.</li>\n</ul>\n</li>\n<li>\n<p><strong>Sentence Embedding</strong>:</p>\n<ul>\n<li>Use the embedding of the [CLS] token as the representation for the entire sentence.</li>\n<li>This embedding can be used for tasks like sentence classification, clustering, etc.</li>\n</ul>\n</li>\n</ol>\n<hr>","frontmatter":{"title":"Bert","author":"Gaurav","time":"25/03/2017","summary":"BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained deep learning model for NLP that understands context by analyzing text bidirectionally. ","article":"NLP","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAIDBP/EABYBAQEBAAAAAAAAAAAAAAAAAAABA//aAAwDAQACEAMQAAABoudspYzh/8QAGRAAAwADAAAAAAAAAAAAAAAAAAESAhEh/9oACAEBAAEFAqRaLMeo2f/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABcRAAMBAAAAAAAAAAAAAAAAAAABEgL/2gAIAQIBAT8Btl6P/8QAGBAAAgMAAAAAAAAAAAAAAAAAAAERIDH/2gAIAQEABj8C1EV//8QAHBAAAgICAwAAAAAAAAAAAAAAAAERITFBUZHw/9oACAEBAAE/IQIru8DTldm6Mn5H/9oADAMBAAIAAwAAABDHD//EABcRAAMBAAAAAAAAAAAAAAAAAAABEVH/2gAIAQMBAT8QiwiP/8QAGBEAAgMAAAAAAAAAAAAAAAAAAAERMWH/2gAIAQIBAT8Qjdmx/8QAHRABAQACAQUAAAAAAAAAAAAAAREAIZExUWHB0f/aAAgBAQABPxAPTzvzKyu4e/nKfaxNHUmaKgU30ypVOGf/2Q=="},"images":{"fallback":{"src":"/static/0342eda530769a70de53a31d0f9b5a38/1e21a/1.jpg","srcSet":"/static/0342eda530769a70de53a31d0f9b5a38/2b49c/1.jpg 200w,\n/static/0342eda530769a70de53a31d0f9b5a38/eee8e/1.jpg 400w,\n/static/0342eda530769a70de53a31d0f9b5a38/1e21a/1.jpg 800w,\n/static/0342eda530769a70de53a31d0f9b5a38/43dfe/1.jpg 1600w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/0342eda530769a70de53a31d0f9b5a38/7ad72/1.webp 200w,\n/static/0342eda530769a70de53a31d0f9b5a38/dff21/1.webp 400w,\n/static/0342eda530769a70de53a31d0f9b5a38/b2a35/1.webp 800w,\n/static/0342eda530769a70de53a31d0f9b5a38/38a76/1.webp 1600w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":450}}}}}},"pageContext":{"slug":"/Blog/Bert/"}},"staticQueryHashes":[],"slicesMap":{}}