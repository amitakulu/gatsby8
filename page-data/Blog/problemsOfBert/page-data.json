{"componentChunkName":"component---src-templates-blog-post-js","path":"/Blog/problemsOfBert/","result":{"data":{"markdownRemark":{"html":"<h3>XLNet:<a href=\"#xlnet-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>XLNet is a fascinating model that builds upon the insights of BERT, and it addresses some of BERT's shortcomings. Let's break down how XLNet works, its advantages over BERT, and how it's trained.</p>\n<h3>1. The Problems with BERT's Training:<a href=\"#1--the-problems-with-bert-s-training-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>Before diving into XLNet, let's first understand the issues it seeks to address in BERT:</p>\n<ul>\n<li>\n<p><strong>Train-Test Skew</strong>: BERT is trained using the masked language model (MLM) objective, where random tokens in a sentence are masked and then predicted. However, during inference, BERT doesn't see these masks and predicts on full sentences. This discrepancy between training and testing conditions can lead to suboptimal performance.</p>\n</li>\n<li>\n<p><strong>Independence of Masks</strong>: When BERT predicts multiple masked tokens, it does so in parallel without considering the interdependence of these tokens. This can lead to nonsensical predictions when the masked tokens are related.</p>\n</li>\n</ul>\n<h3>2. XLNet's Solution: Permutation-based Training:<a href=\"#2--xlnet-s-solution--permutation-based-training-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>XLNet addresses these problems by introducing a permutation-based training mechanism. Instead of masking tokens, XLNet looks at all possible permutations of a sentence and tries to predict one token at a time based on its preceding tokens in that permutation. This way, XLNet learns to handle tokens in various contexts and orders, making it more versatile.</p>\n<h4>Example:</h4>\n<p>Consider the sentence: \"The cat sat on the mat.\"</p>\n<p>XLNet would consider all possible permutations of this sentence, such as:</p>\n<ul>\n<li>\"The, cat, sat, on, the, mat.\"</li>\n<li>\"Mat, the, on, sat, cat, the.\"</li>\n<li>\"On, the, mat, the, cat, sat.\"</li>\n<li>... and so on.</li>\n</ul>\n<p>For each permutation, XLNet would predict one word based on the previous words. For instance, in the permutation \"Mat, the, on, sat, cat, the\", XLNet would first predict \"the\" based on \"Mat,\", then \"on\" based on \"Mat, the\", and so forth.</p>\n<p>This training approach ensures that XLNet considers the context from both the left and right side of a token and learns the dependencies between tokens.</p>\n<h3>3. Advantages of XLNet over BERT:<a href=\"#3--advantages-of-xlnet-over-bert-\" class=\"anchor\">ðŸ”—</a></h3>\n<ul>\n<li>\n<p><strong>Contextualized Training</strong>: Unlike BERT, which only uses left-to-right context during training, XLNet's permutation-based training uses both left and right contexts, making its embeddings richer.</p>\n</li>\n<li>\n<p><strong>Avoids the Train-Test Skew</strong>: Since XLNet doesn't use masking during training, there's no discrepancy between its training and inference conditions.</p>\n</li>\n<li>\n<p><strong>Handles Dependent Masks</strong>: XLNet can correctly predict interdependent tokens due to its sequential prediction mechanism. In cases like \"The Statue of [MASK] in [MASK] [MASK]\", XLNet can correctly predict \"Liberty\" for the first mask and \"New York\" for the next two, understanding the interdependencies between the masked tokens.</p>\n</li>\n</ul>\n<hr>\n<h3>Differences between BERT and XLNet through more examples.<a href=\"#differences-between-bert-and-xlnet-through-more-examples-\" class=\"anchor\">ðŸ”—</a></h3>\n<h3>Train-Test Skew in BERT:<a href=\"#train-test-skew-in-bert-\" class=\"anchor\">ðŸ”—</a></h3>\n<h4><strong>Example 1</strong>:</h4>\n<p>Imagine training a model to complete this sentence:</p>\n<ul>\n<li>\"Elephants have a long ____.\"</li>\n</ul>\n<p>BERT's approach is to mask the word and train on:</p>\n<ul>\n<li>\"Elephants have a long [MASK].\"</li>\n</ul>\n<p>During real-world usage, BERT might see:</p>\n<ul>\n<li>\"Elephants have a long trunk.\"</li>\n</ul>\n<p>The model was trained with masked words, but during real-world scenarios, there are no masks. This difference can cause the model to perform suboptimally.</p>\n<h3>Independence of Masks in BERT:<a href=\"#independence-of-masks-in-bert-\" class=\"anchor\">ðŸ”—</a></h3>\n<h4><strong>Example 2</strong>:</h4>\n<p>Consider a sentence about food:</p>\n<ul>\n<li>\"I like ____ on my ____.\"</li>\n</ul>\n<p>If BERT predicts each mask independently, it might come up with:</p>\n<ul>\n<li>\"I like chocolate on my sandwich.\"</li>\n</ul>\n<p>This doesn't make sense because \"chocolate\" and \"sandwich\" don't usually go together. BERT doesn't consider the relationship between the masked words.</p>\n<h3>XLNet's Permutation-based Training:<a href=\"#xlnet-s-permutation-based-training-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>XLNet's approach is to use different permutations of the sentence to predict each word, considering all the words before it.</p>\n<h4><strong>Example 3</strong>:</h4>\n<p>For the sentence:</p>\n<ul>\n<li>\"Birds can fly.\"</li>\n</ul>\n<p>XLNet might consider permutations like:</p>\n<ol>\n<li>\"Birds, fly, can.\"</li>\n<li>\"Can, fly, birds.\"</li>\n<li>\"Fly, birds, can.\"</li>\n</ol>\n<p>For the permutation \"Fly, birds, can\", XLNet will:</p>\n<ol>\n<li>Predict \"birds\" using \"Fly\" as context.</li>\n<li>Predict \"can\" using \"Fly, birds\" as context.</li>\n</ol>\n<p>This method ensures every word is predicted in the context of other words, both from its left and right.</p>\n<h3>Benefits of XLNet's Approach:<a href=\"#benefits-of-xlnet-s-approach-\" class=\"anchor\">ðŸ”—</a></h3>\n<h4><strong>Example 4</strong>:</h4>\n<p>Consider a tricky sentence:</p>\n<ul>\n<li>\"In the ____ of the night, she went to the ____.\"</li>\n</ul>\n<p>BERT might predict:</p>\n<ul>\n<li>\"In the middle of the night, she went to the store.\"</li>\n</ul>\n<p>XLNet, due to its sequential prediction, might think:</p>\n<ol>\n<li>If \"night\" is the context, then \"middle\" or \"depths\" could fit in the first blank.</li>\n<li>If \"middle of the night\" is the context, then \"garden\" or \"balcony\" might fit the second blank better than \"store\".</li>\n</ol>\n<p>XLNet might predict:</p>\n<ul>\n<li>\"In the middle of the night, she went to the balcony.\"</li>\n</ul>\n<h4><strong>Example 5</strong>:</h4>\n<p>For a sentence about historical events:</p>\n<ul>\n<li>\"The ____ revolution in ____ changed the world.\"</li>\n</ul>\n<p>BERT might independently guess:</p>\n<ul>\n<li>\"The industrial revolution in Africa changed the world.\"</li>\n</ul>\n<p>While \"industrial revolution\" is a valid event, it didn't happen in Africa.</p>\n<p>XLNet, predicting sequentially and considering the context, might think:</p>\n<ol>\n<li>If the context is \"revolution\", then \"industrial\" or \"French\" might fit.</li>\n<li>If \"industrial revolution\" is chosen, then \"Britain\" is a likely next word.</li>\n</ol>\n<p>XLNet might predict:</p>\n<ul>\n<li>\"The industrial revolution in Britain changed the world.\"</li>\n</ul>\n<blockquote>\n<p>While BERT breaks sentences into masked fragments and tries to predict them, XLNet uses the entire context available in different permutations. This makes XLNet more context-aware, allowing it to make predictions that consider the relationships between words.</p>\n</blockquote>\n<hr>\n<h3>Multimodality in Natural Language:<a href=\"#multimodality-in-natural-language-\" class=\"anchor\">ðŸ”—</a></h3>\n<p><strong>Multimodality</strong> refers to the existence of multiple \"modes\" or solutions to a problem. In the context of natural language, it means that there can be multiple correct or sensible outputs for a given input.</p>\n<h4>Simple Example:</h4>\n<p>Consider the prompt:</p>\n<ul>\n<li>\"It's raining, so I'll take my ____.\"</li>\n</ul>\n<p>There are several valid completions:</p>\n<ol>\n<li>\"umbrella.\"</li>\n<li>\"raincoat.\"</li>\n<li>\"boots.\"</li>\n</ol>\n<p>All these completions are correct, making the prediction task multimodal.</p>\n<h3>Challenges with Multimodality:<a href=\"#challenges-with-multimodality-\" class=\"anchor\">ðŸ”—</a></h3>\n<h4>1. <strong>Joint Probability Distribution</strong>:</h4>\n<p>When we talk about predicting words in a sentence, we're essentially estimating the joint probability of words appearing together. In a multimodal setting, there are multiple peaks (high probabilities) in this distribution, which makes the task challenging.</p>\n<h4>2. <strong>Local Optima vs. Global Optima</strong>:</h4>\n<p>Models like BERT, when predicting multiple masked tokens independently, might choose the best prediction for each mask (local optima) but miss out on the best overall combination (global optima).</p>\n<h4>Detailed Example:</h4>\n<p>Let's consider a more complex sentence:</p>\n<ul>\n<li>\"After the long journey, I finally reached ____ and saw the beautiful ____.\"</li>\n</ul>\n<p>If BERT predicts masks independently, it might fill in:</p>\n<ul>\n<li>\"After the long journey, I finally reached <strong>Paris</strong> and saw the beautiful <strong>beach</strong>.\"</li>\n</ul>\n<p>While \"Paris\" and \"beach\" might be the most probable words for their respective masks, the combination doesn't make sense since Paris isn't known for its beaches.</p>\n<p>A globally optimal solution might be:</p>\n<ul>\n<li>\"After the long journey, I finally reached <strong>Hawaii</strong> and saw the beautiful <strong>beach</strong>.\"</li>\n</ul>\n<h4>3. <strong>Challenge in Generation</strong>:</h4>\n<p>In natural language generation tasks, like story generation or dialogue systems, multimodality poses a significant challenge. Given a prompt, there can be various valid continuations, and choosing one that's coherent and contextually relevant becomes difficult.</p>\n<h4>Example:</h4>\n<p>Consider a dialogue system given the prompt:</p>\n<ul>\n<li>\"User: Tell me a story about a mysterious artifact.\"</li>\n</ul>\n<p>The system could respond with:</p>\n<ol>\n<li>\"Once, in the jungles of Africa, an archaeologist discovered a golden statue with eyes made of rare gems...\"</li>\n<li>\"In a quiet town in England, a librarian stumbled upon an ancient book that had the power to...\"</li>\n<li>\"Deep under the ocean, divers found a peculiar stone tablet covered in undecipherable inscriptions...\"</li>\n</ol>\n<p>Each of these stories is a valid response, but the system has to select one or generate a mix that might not be coherent.</p>\n<h3>Solutions:<a href=\"#solutions-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>Models like XLNet try to address the challenge of multimodality by predicting words in a sequence, considering all previous context, which aids in making globally optimal decisions.</p>\n<hr>","frontmatter":{"title":"Problems Of Bert","author":"Gaurav","time":"2021-07-20","summary":"BERT, while powerful, challenges such as computational intensity, difficulty in handling real-time processes, and potential biases from training data. ","article":"NLP","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAABAAC/8QAFgEBAQEAAAAAAAAAAAAAAAAAAQAC/9oADAMBAAIQAxAAAAHB16yilRf/xAAbEAACAQUAAAAAAAAAAAAAAAABEgIAEBETI//aAAgBAQABBQKQaeotzp8l5W//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAcEAABBAMBAAAAAAAAAAAAAAAAAQIRMQMQEjL/2gAIAQEABj8CnoWMblaeCoL1/8QAGhABAAMBAQEAAAAAAAAAAAAAAQARMUFhgf/aAAgBAQABPyElQb4EbLkmWdq/ZsAiWk2utvs//9oADAMBAAIAAwAAABCg7//EABURAQEAAAAAAAAAAAAAAAAAAAAR/9oACAEDAQE/EIj/xAAWEQEBAQAAAAAAAAAAAAAAAAAAESH/2gAIAQIBAT8QxX//xAAdEAEAAgMAAwEAAAAAAAAAAAABABEhMUFRYYGx/9oACAEBAAE/EMTwl09bI8m3sldryXcMQ+g5RVrvdb+THXE7V/sXW/Z2Z//Z"},"images":{"fallback":{"src":"/static/ffae19ecdc0327682d471d7b04c5ad2e/9e885/6.jpg","srcSet":"/static/ffae19ecdc0327682d471d7b04c5ad2e/d9f52/6.jpg 200w,\n/static/ffae19ecdc0327682d471d7b04c5ad2e/10057/6.jpg 400w,\n/static/ffae19ecdc0327682d471d7b04c5ad2e/9e885/6.jpg 800w,\n/static/ffae19ecdc0327682d471d7b04c5ad2e/db60b/6.jpg 1600w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/ffae19ecdc0327682d471d7b04c5ad2e/b9488/6.webp 200w,\n/static/ffae19ecdc0327682d471d7b04c5ad2e/b0f1a/6.webp 400w,\n/static/ffae19ecdc0327682d471d7b04c5ad2e/06861/6.webp 800w,\n/static/ffae19ecdc0327682d471d7b04c5ad2e/04417/6.webp 1600w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":531}}}}}},"pageContext":{"slug":"/Blog/problemsOfBert/"}},"staticQueryHashes":[],"slicesMap":{}}