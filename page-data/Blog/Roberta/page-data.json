{"componentChunkName":"component---src-templates-blog-post-js","path":"/Blog/Roberta/","result":{"data":{"markdownRemark":{"html":"<p>.</p>\n<h3><strong>RoBERTa</strong>:<a href=\"#roberta-\" class=\"anchor\">ðŸ”—</a></h3>\n<p><strong>RoBERTa (Robustly Optimized BERT)</strong> builds upon the foundation laid by BERT, with key modifications in training strategies and data. These changes have made RoBERTa one of the most powerful PLMs available.</p>\n<h3><strong>Key Differences and Improvements over BERT</strong>:<a href=\"#key-differences-and-improvements-over-bert-\" class=\"anchor\">ðŸ”—</a></h3>\n<h3>1. <strong>Training Data</strong>:<a href=\"#1--training-data-\" class=\"anchor\">ðŸ”—</a></h3>\n<p><strong>BERT</strong>: Trained on a dataset that includes text from BooksCorpus and English Wikipedia.</p>\n<p><strong>RoBERTa</strong>: Trained on a much larger dataset, aggregating multiple sources of text.</p>\n<p><strong>NLP Example</strong>: Consider two language models:</p>\n<ul>\n<li><strong>(BERT)</strong>: Trained on a set of 10 classic novels.</li>\n<li><strong>(RoBERTa)</strong>: Trained on those same 10 novels, plus encyclopedias, newspapers, websites, and 90 other novels.</li>\n</ul>\n<p>When asked about a classic literary theme, both might perform well. But for broader topics or nuanced queries, Model B, having seen more diverse content, would likely provide a more informed answer.</p>\n<h3>2. <strong>Next Sentence Prediction (NSP) Objective</strong>:<a href=\"#2--next-sentence-prediction--nsp--objective-\" class=\"anchor\">ðŸ”—</a></h3>\n<p><strong>BERT</strong>: BERT's training involved predicting if one sentence naturally follows another in the text.</p>\n<p><strong>RoBERTa</strong>: This NSP task was removed; RoBERTa focused solely on predicting masked words in sentences.</p>\n<p><strong>NLP Example</strong>:</p>\n<ul>\n<li><strong>(BERT)</strong>: Given sentences \"She went to the bakery.\" and \"She bought a cake.\", it predicts if the second sentence is a logical continuation of the first.</li>\n<li><strong>(RoBERTa)</strong>: Focuses only on filling in blanks, like \"She went to the ____.\" without worrying about the next sentence.</li>\n</ul>\n<p>The RoBERTa team found that just focusing on the fill-in-the-blank task (without the next sentence prediction) resulted in as good, if not better, performance on downstream tasks.</p>\n<h3>3. <strong>Batch Size and Masking Strategy</strong>:<a href=\"#3--batch-size-and-masking-strategy-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>RoBERTa employed larger batch sizes and changed how words were masked for prediction during training.</p>\n<p><strong>NLP Example</strong>:</p>\n<ul>\n<li>\n<p><strong>BERT</strong>: In every training iteration, the same words might be masked out for prediction. For instance, in the sentence \"The cat sat on the mat.\", \"cat\" and \"mat\" might always be the masked words.</p>\n</li>\n<li>\n<p><strong>RoBERTa</strong>: The words that are masked for prediction vary with each pass. So, in one iteration \"cat\" might be masked, while in another \"sat\" and \"on\" might be masked. This dynamic approach offers the model varied training scenarios.</p>\n</li>\n</ul>\n<h3><strong>Performance of RoBERTa</strong>:<a href=\"#performance-of-roberta-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>Post-training, when used on tasks like question-answering or sentiment analysis:</p>\n<ul>\n<li><strong>BERT</strong>: Might predict the sentiment of the sentence \"I love sunny days!\" as positive.</li>\n<li><strong>RoBERTa</strong>: Given its refined training, might predict the sentiment with higher confidence and possibly more accurately in nuanced scenarios, such as \"Sunny days make the unbearable heat worse.\" where the sentiment might be negative despite the presence of a typically positive word like \"sunny\".</li>\n</ul>\n<h3><strong>Transitioning from BERT to RoBERTa</strong>:<a href=\"#transitioning-from-bert-to-roberta-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>Since the underlying architecture is the same, using RoBERTa in place of BERT is straightforward in NLP tasks.</p>\n<ul>\n<li>If a system uses BERT for named entity recognition, identifying \"Barack Obama\" as a person in the text, switching to RoBERTa might yield similar or better accuracy without changing the system's structure.</li>\n</ul>\n<hr>\n<h3><strong>Static Masking (as in BERT)</strong>:<a href=\"#static-masking--as-in-bert--\" class=\"anchor\">ðŸ”—</a></h3>\n<p>In the original BERT training process, a certain percentage of the input tokens were selected to be masked out and then predicted by the model. This masking was done before training starts, and the same words were masked out in each sentence throughout the entire training process.</p>\n<p><strong>Example</strong>:\nSuppose we're training BERT on the following two sentences multiple times:</p>\n<ol>\n<li>\"She reads a book.\"</li>\n<li>\"They play soccer.\"</li>\n</ol>\n<p>If \"reads\" and \"book\" were chosen to be masked in the first sentence, and \"play\" in the second sentence, then in every training iteration, the model would see:</p>\n<ol>\n<li>\"She [MASK] a [MASK].\"</li>\n<li>\"They [MASK] soccer.\"</li>\n</ol>\n<p>The model would always try to predict \"reads\" and \"book\" for the first sentence and \"play\" for the second, across all training epochs.</p>\n<h3><strong>Dynamic Masking (as in RoBERTa)</strong>:<a href=\"#dynamic-masking--as-in-roberta--\" class=\"anchor\">ðŸ”—</a></h3>\n<p>In RoBERTa, the tokens to be masked are chosen dynamically in each epoch (or pass over the data). This means that in different training iterations, different words can be masked out.</p>\n<p><strong>Example</strong>:\nUsing the same two sentences:</p>\n<ol>\n<li>\"She reads a book.\"</li>\n<li>\"They play soccer.\"</li>\n</ol>\n<p>In the first training iteration, RoBERTa might mask and try to predict:</p>\n<ol>\n<li>\"She [MASK] a book.\" (predict \"reads\")</li>\n<li>\"They [MASK] soccer.\" (predict \"play\")</li>\n</ol>\n<p>In the second iteration, it might mask:</p>\n<ol>\n<li>\"She reads a [MASK].\" (predict \"book\")</li>\n<li>\"They [MASK] [MASK].\" (predict \"play\" and \"soccer\")</li>\n</ol>\n<p>And in another iteration:</p>\n<ol>\n<li>\"She [MASK] a [MASK].\" (predict \"reads\" and \"book\")</li>\n<li>\"They play [MASK].\" (predict \"soccer\")</li>\n</ol>\n<p>This dynamic approach means the model gets a more diverse set of challenges during training, as it doesn't always predict the same masked words for the same sentences. This can lead to a better, more generalized understanding of language.</p>\n<p><strong>Why is this Beneficial?</strong>\nBy dynamically changing the masked words, the model learns to understand the context from various angles. It doesn't get \"used to\" one fixed pattern of missing words. This dynamic challenge can lead to a richer understanding of sentence structures and relationships between words, potentially enhancing the model's performance on downstream tasks.</p>\n<hr>\n<h3>DistilBERT<a href=\"#distilbert\" class=\"anchor\">ðŸ”—</a></h3>\n<h3><strong>The Problem with Large Models</strong>:<a href=\"#the-problem-with-large-models-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>Models like BERT and RoBERTa, while being extremely powerful, come with a significant computational cost. They have a huge number of parameters (110 million for BERT-base and 340 million for BERT-large), which means they require a substantial amount of computational power to make predictions.</p>\n<p><strong>Example</strong>: Imagine having an extremely detailed map of a city. While this map provides a lot of information, it's heavy and cumbersome to carry around. If you're hiking and need to move quickly, this detailed map might slow you down.</p>\n<p>Similarly, in real-world applications, especially those that need real-time processing like mobile applications or edge devices, using BERT or its variants can be impractical. It's like trying to run a high-end video game on an old computer â€“ the game might not run smoothly or might not run at all.</p>\n<h3><strong>Knowledge Distillation</strong>:<a href=\"#knowledge-distillation-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>To address the computational challenge of large models, the concept of knowledge distillation is employed. Here's how it works:</p>\n<ol>\n<li><strong>Teacher Model</strong>: This is the original, large model (like BERT). It's been trained and is knowledgeable, but it's also computationally heavy.</li>\n<li><strong>Student Model</strong>: A smaller, lighter model that we want to train.</li>\n<li>The student model is trained not just on the original data but also using the outputs from the teacher model. The goal is to make the student model mimic the behavior of the teacher model as closely as possible.</li>\n</ol>\n<h3><strong>DistilBERT</strong>:<a href=\"#distilbert-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>Building on the idea of knowledge distillation, DistilBERT was introduced by researchers at Hugging Face. It's a distilled version of BERT â€“ smaller, faster, but still retaining most of the performance capabilities of BERT.</p>\n<p><strong>Specifications</strong>:</p>\n<ul>\n<li>DistilBERT is 40% smaller than BERT.</li>\n<li>It is 60% faster in processing.</li>\n<li>Despite its reduced size, it retains 97% of BERT's performance on downstream tasks.</li>\n</ul>\n<h3><strong>Using DistilBERT</strong>:<a href=\"#using-distilbert-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>Given the compatibility of DistilBERT with the Transformers library, switching from BERT to DistilBERT in applications is quite straightforward. If you have an application that uses BERT, transitioning to DistilBERT often involves just changing the model name in the loading function. This ensures that you get a faster, more efficient model without a complete overhaul of your existing system.</p>\n<hr>\n<h3><strong>Knowledge Distillation in NLP</strong><a href=\"#knowledge-distillation-in-nlp\" class=\"anchor\">ðŸ”—</a></h3>\n<p>Imagine you have a huge dataset containing countless sentences, and you want to train a model to predict the next word in a given sentence.</p>\n<ol>\n<li>\n<p><strong>Teacher Model (BERT)</strong>:</p>\n<ul>\n<li>You first train a large and comprehensive model (like BERT) on this dataset. After extensive training, this model becomes highly proficient at predicting the next word.</li>\n<li>For example, given the sentence \"She sold sea shells by the...\", BERT predicts \"seashore\" with a high probability.</li>\n<li>However, due to its size, BERT takes a considerable amount of time and computational power to produce these predictions.</li>\n</ul>\n</li>\n<li>\n<p><strong>Student Model (DistilBERT)</strong>:</p>\n<ul>\n<li>You now want a faster model that can make near-similar predictions but consume less computational power. So, you introduce a smaller model (like DistilBERT).</li>\n<li>Instead of training this model solely on the original sentences, you also expose it to the predictions made by BERT.</li>\n</ul>\n</li>\n<li>\n<p><strong>Training Process</strong>:</p>\n<ul>\n<li>You feed a sentence to both BERT and DistilBERT: \"The early bird catches the...\".</li>\n<li>BERT predicts \"worm\" with a probability of 0.95, \"prey\" with 0.04, and other words with lower probabilities.</li>\n<li>During DistilBERT's training, it's not only trying to predict the next word based on the sentence but also trying to match its prediction probabilities closely to BERT's. So, it tries to adjust itself to predict \"worm\" with a probability close to 0.95 and \"prey\" close to 0.04.</li>\n<li>Over many such sentences and predictions, DistilBERT learns to mimic BERT's behavior closely.</li>\n</ul>\n</li>\n<li>\n<p><strong>Outcome</strong>:</p>\n<ul>\n<li>After this training process, when you feed a new sentence to DistilBERT, it predicts the next word in a manner very similar to BERT but in a shorter time and using less computational power.</li>\n<li>For instance, for the sentence \"Rainbows appear after it...\", both BERT and DistilBERT might predict \"rains\" as the next word. While BERT might take 2 seconds for this prediction, DistilBERT might do it in 0.5 seconds.</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h3><strong>ALBERT: A Brief Introduction</strong><a href=\"#albert--a-brief-introduction\" class=\"anchor\">ðŸ”—</a></h3>\n<p>ALBERT is a variant of BERT designed to be more efficient in terms of computational power and memory. It addresses the issues in BERT by introducing novel design changes and training strategies.</p>\n<hr>\n<h3><strong>Word Embedding Decomposition</strong><a href=\"#word-embedding-decomposition\" class=\"anchor\">ðŸ”—</a></h3>\n<p>In most deep NLP models, each word is linked to a dense vector called an embedding. These embeddings are stored in a table.</p>\n<h4><strong>Problem with BERT's Embeddings</strong>:</h4>\n<p>BERT uses embeddings of large dimensions (like 768). So, for a vocabulary of 30,000 words, you'd have a table of size <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>30</mn><mo separator=\"true\">,</mo><mn>000</mn><mo>Ã—</mo><mn>768</mn></mrow><annotation encoding=\"application/x-tex\">30,000 \\times 768</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8389em;vertical-align:-0.1944em;\"></span><span class=\"mord\">30</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">000</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">Ã—</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">768</span></span></span></span></span>. This is computationally expensive.</p>\n<h4><strong>ALBERT's Solution</strong>:</h4>\n<p>ALBERT uses a two-step embedding process:</p>\n<ol>\n<li>Retrieve a smaller embedding (e.g., 128 dimensions) for the word.</li>\n<li>Expand this smaller embedding to the desired size (e.g., 768 dimensions) using a linear layer.</li>\n</ol>\n<p><strong>Example</strong>:\nThink of the smaller embedding as a compressed ZIP file. When needed, ALBERT \"unzips\" it to its original size using a specific method (the linear layer). This approach allows ALBERT to store information more efficiently.</p>\n<hr>\n<h3><strong>Parameter Sharing between Transformer Layers</strong><a href=\"#parameter-sharing-between-transformer-layers\" class=\"anchor\">ðŸ”—</a></h3>\n<p>Transformer models, like BERT, use multiple layers to process input data. Each layer has its own set of parameters, making the model large and heavy.</p>\n<h4><strong>ALBERT's Approach</strong>:</h4>\n<p>ALBERT uses the same set of parameters across all its layers. Instead of each layer having a unique transformation, they all apply the same transformation repeatedly, but in a manner that's effective for the task.</p>\n<p><strong>Example</strong>:\nImagine reading a book, and with each subsequent reading (or layer), you're wearing a different colored lens. BERT uses different lenses for each reading, while ALBERT uses the same lens repeatedly, but fine-tunes how it reads through it.</p>\n<hr>\n<h3><strong>Sentence Order Prediction (SOP)</strong><a href=\"#sentence-order-prediction--sop-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>BERT used a Next-Sentence Prediction (NSP) objective for training. ALBERT found this less effective and replaced it with Sentence Order Prediction (SOP).</p>\n<h4><strong>What is SOP?</strong></h4>\n<p>Given two consecutive segments of text, the model predicts their correct order.</p>\n<p><strong>Example</strong>:</p>\n<ol>\n<li>Sentence A: \"After the movie, they went to a cafe.\"\nSentence B: \"John and Mary decided to watch a movie.\"</li>\n</ol>\n<p>The correct order should be B then A. ALBERT trains itself on such examples to understand context and coherence better than BERT's NSP.</p>\n<hr>\n<h3><strong>Benefits of ALBERT</strong>:<a href=\"#benefits-of-albert-\" class=\"anchor\">ðŸ”—</a></h3>\n<ol>\n<li><strong>Reduced Size</strong>: By sharing parameters and using embedding decomposition, ALBERT significantly reduces its size compared to BERT.</li>\n<li><strong>Better Training Objective</strong>: SOP provides a deeper understanding of context and coherence.</li>\n<li><strong>Scalability</strong>: Despite its reduced size, ALBERT can be scaled up effectively and can outperform larger models with fewer parameters.</li>\n</ol>\n<h3><strong>Using ALBERT in NLP Applications</strong>:<a href=\"#using-albert-in-nlp-applications-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>Given its architecture compatibility with BERT, transitioning to ALBERT in NLP tasks is straightforward. It's like swapping out a heavy textbook (BERT) for a concise guidebook (ALBERT) that has the same essential content but is easier to carry around.</p>\n<hr>\n<p>Parameter sharing across layers is a technique where the weights (parameters) of one layer are used in another layer instead of having separate weights for each layer. This reduces the number of unique parameters in the model, leading to a smaller model size.</p>\n<h3>Without Parameter Sharing:<a href=\"#without-parameter-sharing-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>Imagine a toy neural network with two layers: Layer 1 and Layer 2.</p>\n<ul>\n<li><strong>Layer 1</strong> has weights: <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">â€‹</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span></li>\n<li><strong>Layer 2</strong> has weights: <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">â€‹</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span></li>\n</ul>\n<p>When we pass input <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span></span> through the network:</p>\n<ol>\n<li>The output of Layer 1 is <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>O</mi><mn>1</mn></msub><mo>=</mo><mi>X</mi><mo>Ã—</mo><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">O_1 = X \\times W_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">â€‹</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">Ã—</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">â€‹</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span></li>\n<li>The output of the entire network is <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>O</mi><mn>2</mn></msub><mo>=</mo><msub><mi>O</mi><mn>1</mn></msub><mo>Ã—</mo><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">O_2 = O_1 \\times W_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">â€‹</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">â€‹</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">Ã—</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">â€‹</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span></li>\n</ol>\n<p>In this scenario, Layer 1 and Layer 2 have distinct sets of weights.</p>\n<h3>With Parameter Sharing:<a href=\"#with-parameter-sharing-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>Now, let's assume we're sharing parameters across these two layers.</p>\n<ul>\n<li>Both <strong>Layer 1</strong> and <strong>Layer 2</strong> use the same weights: <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span></li>\n</ul>\n<p>When we pass input <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span></span> through the network:</p>\n<ol>\n<li>The output of Layer 1 is <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>O</mi><mn>1</mn></msub><mo>=</mo><mi>X</mi><mo>Ã—</mo><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">O_1 = X \\times W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">â€‹</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">Ã—</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span></li>\n<li>The output of the entire network is <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>O</mi><mn>2</mn></msub><mo>=</mo><msub><mi>O</mi><mn>1</mn></msub><mo>Ã—</mo><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">O_2 = O_1 \\times W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">â€‹</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">â€‹</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">Ã—</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span></li>\n</ol>\n<p>Here, both layers are using the same set of weights <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span>. So, we've effectively halved the number of unique weights in our model.</p>\n<h3>In the Context of ALBERT:<a href=\"#in-the-context-of-albert-\" class=\"anchor\">ðŸ”—</a></h3>\n<p>For models like ALBERT, which is based on the Transformer architecture, the concept is applied to the multiple layers of the Transformer. Instead of each Transformer layer having its own set of weights, they all share the same set of weights.</p>\n<p>For example, in a typical BERT model with 12 layers, each layer has its own set of parameters. So, if each layer has 10 million parameters, the total is 120 million parameters.</p>\n<p>But in ALBERT with parameter sharing, all 12 layers could share the same 10 million parameters, leading to significant savings in terms of memory and computational requirements.</p>\n<p>This parameter sharing approach allows ALBERT to have fewer parameters than BERT for a given model depth, making it lighter and potentially faster. However, one trade-off is that ALBERT might have less capacity to learn diverse features across layers, since all layers are constrained to use the same parameters. Despite this, ALBERT has shown competitive performance on various NLP benchmarks compared to BERT.</p>\n<hr>\n<p>There are a few reasons why parameter sharing across layers in ALBERT can still be effective:</p>\n<ol>\n<li>\n<p><strong>Different Inputs to Each Layer</strong>: Even though the parameters are shared across layers, the input to each layer is different. The input to the second layer is the output from the first layer, the input to the third layer is the output from the second layer, and so on. As a result, each layer operates on increasingly abstract representations of the original input, even if the transformation at each layer is the same.</p>\n</li>\n<li>\n<p><strong>Residual Connections</strong>: The Transformer architecture, on which both BERT and ALBERT are based, includes residual connections. This means that the output of each layer is the sum of its input and its transformed input. This allows information to flow through the network without being \"diluted\" by the repeated application of the same transformation.</p>\n</li>\n<li>\n<p><strong>Training Dynamics</strong>: The shared parameters are updated based on gradients from all layers. This means that during training, the shared parameters are being optimized to be useful across all layers. They are effectively being \"averaged\" over all layers, which might lead them to capture more general features that are useful at multiple levels of abstraction.</p>\n</li>\n<li>\n<p><strong>Regularization Effect</strong>: Sharing parameters can be seen as a form of regularization. By constraining multiple layers to use the same parameters, the model is forced to find representations that are useful across multiple layers. This can prevent the model from overfitting to the training data.</p>\n</li>\n<li>\n<p><strong>Empirical Performance</strong>: Despite the concerns about shared parameters limiting the model's capacity, ALBERT has achieved competitive performance on various NLP benchmarks compared to models like BERT. This suggests that, in practice, the parameter-sharing approach is effective for many tasks.</p>\n</li>\n</ol>\n<hr>","frontmatter":{"title":"Roberta, DistilBert And Albert","author":"Gaurav","time":"25/03/2017","summary":"RoBERTa refines BERT's training, DistilBERT offers compactness and speed, and ALBERT emphasizes scalability with fewer parameters. ","article":"NLP","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAwQA/8QAFgEBAQEAAAAAAAAAAAAAAAAAAQIE/9oADAMBAAIQAxAAAAEEnozB6zSf/8QAGRAAAwEBAQAAAAAAAAAAAAAAAAECIRES/9oACAEBAAEFAsZ5Q5Lol9Uxn//EABURAQEAAAAAAAAAAAAAAAAAAAAR/9oACAEDAQE/AVf/xAAXEQADAQAAAAAAAAAAAAAAAAAAARIh/9oACAECAQE/AXiKP//EABkQAAEFAAAAAAAAAAAAAAAAABARICExQf/aAAgBAQAGPwKcYtD/xAAZEAADAQEBAAAAAAAAAAAAAAAAAREhMUH/2gAIAQEAAT8hVpjikmtvpplOMUoqNUIdNs//2gAMAwEAAgADAAAAENQ//8QAFhEBAQEAAAAAAAAAAAAAAAAAAREQ/9oACAEDAQE/EKBcf//EABYRAQEBAAAAAAAAAAAAAAAAAAABMf/aAAgBAgEBPxDQlP/EABoQAQEBAQEBAQAAAAAAAAAAAAERIQBhMUH/2gAIAQEAAT8QILfBrfes3QreO4n8jnPIQnDzgWECDTCcHpu49//Z"},"images":{"fallback":{"src":"/static/791faa62a2680cecd6331e3a93280138/3b307/8.jpg","srcSet":"/static/791faa62a2680cecd6331e3a93280138/d9f52/8.jpg 200w,\n/static/791faa62a2680cecd6331e3a93280138/0132d/8.jpg 400w,\n/static/791faa62a2680cecd6331e3a93280138/3b307/8.jpg 800w,\n/static/791faa62a2680cecd6331e3a93280138/38d08/8.jpg 1600w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/791faa62a2680cecd6331e3a93280138/b9488/8.webp 200w,\n/static/791faa62a2680cecd6331e3a93280138/47294/8.webp 400w,\n/static/791faa62a2680cecd6331e3a93280138/89c0d/8.webp 800w,\n/static/791faa62a2680cecd6331e3a93280138/03bfd/8.webp 1600w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":533}}}}}},"pageContext":{"slug":"/Blog/Roberta/"}},"staticQueryHashes":[],"slicesMap":{}}